% Thesis abstract
%
% Developed for my Master Thesis at Maastricht University.
% Based on Eugenio Senes's template at the University of Torino.
%
% By Joeri Hermans (joeri@joerihermans.com)
%
% Released under an MIT license. Share, modify and enjoy, but quote the author!

\newpage
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

Speeding up gradient based methods has been a subject of interest over the past years with many practical applications, especially with respect to Deep Learning. Despite the fact that many optimizations have been done on a hardware level, the convergence rate of very large models remains problematic. Therefore, data parallel methods next to mini-batch parallelism have been suggested~\cite{dean2012large, ho2013more, hadjis2016omnivore, recht2011hogwild, louppe2010zealous, jiang2017heterogeneity, zhang2015deep} to further decrease the training time of parameterized models using gradient based methods. Nevertheless, asynchronous optimization was considered too unstable for practical purposes due to a lacking understanding of the underlaying mechanisms.\\

Recently, a theoretical contribution has been made~\cite{implicitmomentum} which defines asynchronous optimization in terms of (implicit) \emph{momentum} due to the precense of a queing model of gradients based on past parameterizations. This thesis mainly builds upon this work, and~\cite{zhang2015deep} to construct a better understanding why asynchronous optimization shows proportionally more divergent behaviour when the number of parallel workers increases, and how this affects existing distributed optimization algorithms.\\

Furthermore, using our redefinition of \emph{parameter staleness}, we construct two \emph{novel} techniques for asynchronous optimization, i.e., \textsc{agn} and \textsc{adag}. In this work we show that these methods outperform existing methods, and are more robust to (distributed) hyperparameterization contrary to existing distributed optimization algorithms such as \textsc{downpour}~\cite{dean2012large}, \textsc{(a)easgd}~\cite{zhang2015deep}, and \textsc{dynsgd}~\cite{jiang2017heterogeneity}. Additionally, this thesis presents several smaller contributions. First, we show that the convergence rate of \textsc{easgd} derrived algorithms is impaired by an \emph{equilibrium condition}. However, this equilibrium condition makes sure that \textsc{easgd} does not not overfit quickly. Finally, we introduce a new metric, \emph{temporal efficiency}, to evaluate distributed optimization algorithms against each other.
