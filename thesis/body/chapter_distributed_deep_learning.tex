% Chapter on Distributed Deep Learning.
%
% Developed for my Master Thesis at Maastricht University.
% Based on Eugenio Senes's template at the University of Torino.
%
% By Joeri Hermans (joeri@joerihermans.com)
%
% Released under an MIT license. Share, modify and enjoy, but quote the author!

\chapter{Distributed Deep Learning}
\label{chapter:distributed_deep_learning}

In this chapter, we introduce several concepts and techniques related to Distributed Deep Learning on which this works builts upon. We start in Section~\ref{sec:ddl_introduction} with a recap of all methods and techniques we have discussed in Chapter~\ref{chapter:introduction}. Afterwards, we continue with a discussion of synchronous followed by an examination of asynchronous optimization methods such as \textsc{downpour} and closely related extensions. Furthermore, we address several issues such as \emph{asynchrony induced momentum} which are related to asynchronous optimization. We also consider several approaches which provide a possible solution to these issues.

\section{Introduction}
\label{sec:ddl_introduction}

For all practical applications, Stochastic Gradient Descent (\textsc{sgd}) and derrivatives are the best tools from the numerical optimization toolbox for neural networks. However, applying \textsc{sgd} in its pure form, that is, updating the parameters after evaluating a training sample, is a computationally intensive process. An initial approach for speeding up \textsc{sgd} in terms of convergence with respect to training time was to compute the gradients of several samples, a \emph{mini-batch}, and average them. This approach has several advantages, the first being that a larger mini-batch will result in less noisy updates, as more ``evidence'' of the surrounding error space will provide a better gradient update. The second advantage being the increased computational parallelism, since all sub-gradients (gradients of the training samples in the mini-batch) are based upon the same parametrization of the model. As a result, the parallelization of the gradient computation is quite straightforward. For instance, for every training sample in a mini-batch, one could allocate a thread, a process, or even a different machine (see Figure~\ref{fig:distributed_mini_batch_parallelism}) to compute the gradients in parallel. However, a blocking mechanism is required in order to sum all gradients, average them, and finally update the parametrization of the model. This process is depicted in Figure~\ref{fig:minibatch_data_parallelism}. As discussed in Chapter~\ref{chapter:introduction}, mini-batch parallelism is an instance of \emph{synchronous data parallelism}. Although many synchronous optimization schemes share a similar structure, we discuss other instances of synchronous data parallelism in particular in Section~\ref{sec:synchronous_data_parallelism} since these optimization schemes incorperate gradients and worker parameterizations into the central variable differently compared to mini-batch parallelism.\\

Nevertheless, a significant, but albeit technical issue in synchronous optimization is when a single or multiple workers are slowed down for some reason, e.g., due to high CPU load, or bandwidth consumption, other workers will have to wait before they can continue with step $t + 1$. As a result, the allocated resources are not fully utilized. This particular issue is known in literature as the \emph{straggler} problem. However, this problem can be mitigated with by using a \emph{homogeneous} hardware configuration. For instance, when one would employ 2 different GPU's running at different clock speeds, a \emph{heteregenous} hardware configuration, then the CPU will always have to wait for a particulur GPU since it runs at a lower clock speed causing the complete training procedure to be slowed down\footnote{A chain is only as strong as its weakest link.}. Furthermore, we could argue that there is a limit to synchronous data parallelism because simply \emph{adding more workers to the problem implicitly increases the size of the mini-batch}. As a result, when applying synchronous data parallelism, one is not parallelizing gradient descent in a typical sense, but rather parallelizing the computations within a step. Of course, one could even increase the parallelism within synchronous data parallelism even further by applying model parallelism as dicussed briefly in Section~\ref{sec:intro_model_parallelism}. Nevertheless, while such an implementation is definitly possible, it might be more cost-aware from an economical perspective to just let the model train for a longer period of the compared to actually implementing the training procedure described above. Furthermore, even with if one would implement said training method, there is still a limit to the amount parallelism due to the structure of the computation graph, and communication cost between devices which have to be taken into account. This of course begs the question if it is actually possible to push the limits of asynchrony, and thereby reducing the training time even further. Or from a different perspective, is there a more trivial method besides implementing the above training procedure to reduce the training time.\\

Several approaches~\cite{dean2012large, ho2013more, cipar2013solving, recht2011hogwild, zhang2015deep, louppe2010zealous, jiang2017heterogeneity} have been suggested over the past years which accomplish exactly this. All these methods are instances of \emph{asynchronous data parallelism}, discussed in Section~\ref{sec:intro_data_parallelism}. In contrast to synchronous data parallelism, asynchronous methods can be identified by the \emph{absence} of a blocking mechanism which is present in synchronous data parallelism. Despite the fact that this method resolves the waiting time induced by stragglers, it introduces a closely related but persistent issue. More formally, the \emph{staleness} issue is due to the fact that all $n$ workers update the central variable in an asynchronous fashion. Meaning, from the moment a worker $k$ is done computing an update $\Delta\theta^k$ based upon parameterization of the central variable $\tilde{\theta}_{t}$, it will commit $\Delta\theta^k$ to the parameter server, and afterwards continue with the next mini-batch. Because of this behaviour, it is possible that a number of central variable updates $\tau$ occurred during the time worker $k$ was computing $\Delta\theta^k$. As a result, instead of obtaining $\tilde{\theta}_{t+1}$ by applying $\Delta\theta^k$, worker $k$ is actually applying $\Delta\theta^k$ to $\tilde{\theta}_{t+\tau}$. Which is not ideal, since $\Delta\theta^k$ is based on parametrization $\tilde{\theta}_t$. From~\cite{implicitmomentum} we know that increasing the number of workers actually increases the amount of staleness $\tau$ since $\mathbf{E}[\tau] = (n - 1)$ under a \emph{homogeneous} hardware configuration and a simple queuing model\footnote{With a simple queuing model we intent that updates $\Delta\theta^k$ are incorperated into the central variable in a queuing fashion.}. This result is validated empirically in one of our experiments, shown in Figure~\ref{fig:staleness_distribution}.\\

A side-effect of updating the central variable with stale updates in an asynchronous fashion, is that stale updates carry information about previous states of the central variable. Which is to be expected since worker updates are based on older parameterizations of the central variable. Using this intuition, the authors in~\cite{implicitmomentum} show formally that in a regular asynchronous \textsc{sgd} setting, like \textsc{downpour}, stale updates behave like \emph{momentum}. Furthermore, their formalization can even describe the amount of \emph{implicit momentum}, described in Equation~\ref{eq:implicit_momentum}, which is present in an asynchronous optimization procedure. Furthermore, when applying (Nesterov) momentum in a traditional optimization setting, i.e., sequential parameter updates, one needs to specify the amount of momentum. This is usually denoted by a hyperparameter $\mu$. However, we would like to note that in an asynchronous setting, the hyperparameter $\mu_s$ from Equation~\ref{eq:implicit_momentum}, is not explicitly defined in the optimizer, but arises from the number of asynchronous workers. As a result, Equation~\ref{eq:implicit_momentum} is merely descriptive.

\begin{equation}
  \label{eq:implicit_momentum}
  \mu_s = \Bigg(1 - \frac{1}{n}\Bigg)
\end{equation}

In a previous paragraph we said that there is a limit to mini-batch parallelism, since adding more workers to the problem implicitly increases the size of a mini-batch. However, we observe in Figure~\ref{fig:staleness_distribution} in accordance with~\cite{implicitmomentum}, that there might be a limit to asynchronous optimization as well. However, the authors in~\cite{implicitmomentum} assume that gradients coming from the workers are not adaptive\footnote{Meaning, they are not modified with respect to some (hyper)parameter.}, as can be detucted from their proof. The question begs, can we push asynchronous optimization even further? We answer this question in Chapter~\ref{chapter:accumulated_gradient_normalization}, and Chapter~\ref{chapter:asynchronous_distributed_adaptive_gradients}, by introducing new techniques using a better, and more intuitive understanding of parameter staleness.

\begin{figure}
  \centering
  \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{resources/images/staleness_10}
    \caption{$n = 10$}
    \label{fig:staleness_distribution_10}
  \end{subfigure}
  \begin{subfigure}{.45\textwidth}
     \centering
     \includegraphics[width=\linewidth]{resources/images/staleness_20}
     \caption{$n = 20$}
    \label{fig:staleness_distribution_20}
  \end{subfigure}
  \begin{subfigure}{.45\textwidth}
     \centering
     \includegraphics[width=\linewidth]{resources/images/staleness_30}
     \caption{$n = 30$}
     \label{fig:staleness_distribution_30}
  \end{subfigure}
  \begin{subfigure}{.45\textwidth}
     \centering
     \includegraphics[width=\linewidth]{resources/images/staleness_60}
     \caption{$n = 60$}
     \label{fig:staleness_distribution_60}
  \end{subfigure}
  \caption{These figures show the staleness distribution during a training procedure using a differing number of parallel workers. For every central variable update, we record the staleness $\tau$, and increment the number of occurences of this particular staleness by 1. Thus effectievely building a histogram showing the staleness distribution during the training. With this, we experimentally validate the observations of~\cite{implicitmomentum} that $\mathbf{E}[\tau] = (n - 1)$. Furthermore, the claim that staleness is geometrically distributed during training also holds (right half of the distribution).}
  \label{fig:staleness_distribution}
\end{figure}

\section{Synchronous Data Parallelism}
\label{sec:synchronous_data_parallelism}

\subsection{Model Averaging}
\label{sec:model_averaging}

As the name suggests, model averaging optimizes the central variable by simply averaging the parametrizations of the workers after $\lambda$ (which could be the amount of data in a single epoch) steps until all data has been consumed. As mentioned before, hyperparameter $\lambda$ denotes the number of \emph{local} steps that have to be performed before the results are communicated with the parameter server. The optimization procedure in the worker and parameter server are quite straightforward, and are described in Equation~\ref{eq:model_averaging_worker_update} and Equation~\ref{eq:model_averaging_ps_update} respectively. However, Equation~\ref{eq:model_averaging_worker_update} has the disadvantage that a lot of communication with the parameter server has to be performed, since after every \emph{local} worker update all parameters have to be shipped to the parameter server to apply Equation~\ref{eq:model_averaging_ps_update}. Furthermore, this approach has several other issues that will be discussed later. But for now we can resolve this issue by delaying a commit to the parameter server by doing several \emph{local} ($\lambda$) updates before sending $\theta^k_t$ to the parameter server, this is shown in Algorithm~\ref{algo:model_averaging_worker}.

\begin{equation}
  \label{eq:model_averaging_worker_update}
  \theta^k_{t+1} = \theta^k_t - \eta_t \odot \nabla_\theta \mathcal{L}(\theta^k_t;\textbf{x}^k_t;\textbf{y}^k_t)
\end{equation}

\begin{equation}
  \label{eq:model_averaging_ps_update}
  \tilde{\theta}_{t+1} = \frac{1}{n}\sum^n_{i=1} \theta^i_t
\end{equation}

Contrary to other synchronous methods, model averaging does not reduce the training time in general. In fact, it requires more resources to achieve the same results since the central variable is set to be the average of all workers. This is shown in Figure~\ref{fig:model_averaging_slow} (a) since all workers follow the same first-order path, synchronize, and average the parameters after $\lambda$ steps to start again from the averaged parameterization, which is in this case the central variable. However, what happens if we initialize the parameterizations of the workers randomly? At first, all workers will do some work locally, but after $\lambda$ steps, the parametrizations of the workers are averaged. As a result, all workers share the same parameterization in the next step, which brings us again to our initial scenario as shown in Figure~\ref{fig:model_averaging_slow} (b). Furthermore, when applying random initialization, we could say a ``warmup'' period is required since all workers need to converge a particular solution before convergence can take place. This intuition is strenghtened in Figure~\ref{fig:model_averaging_intuition}.

\begin{figure}[H]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{resources/images/model_averaging_identical_starting_point}
    \caption{Identical initialization}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{resources/images/model_averaging_cf_10}
    \caption{Random initialization}
  \end{subfigure}
  \caption{In this Figure we show the difference between identical initialization (a), and random initialization (b). In essence, both methods require roughly the same amount of time as a sequential optimization algorithm, i.e., not distributed, while using more resources. However, the difference here is that using random initialization requires a ``warmup'' period (a single parameter server update), before the actual optimization process can start. In order to simulate the stochastic noise of mini-batch gradient descent, we added a noise term to our gradient computations which was sampled from $X \sim \mathcal{N}(\mu = 0,\,\sigma^{2} = 10.0)$ to ensure that some deviation from the central variable was possible.}
  \label{fig:model_averaging_slow}
\end{figure}

\newpage

\begin{algorithm}[H]
  \caption{Describes the worker procedure with local worker exploration. The worker will be identified with a certain index $k$ and will be initialized by assigning the central variable ($\tilde{\theta}_t$) to the worker, or the worker variable can be randomized at the start of the optimization procedure. Furthermore, we introduce a hyperparameter $\lambda$, which is the number of local updates that have to be performed in order the worker parameterization $\theta^k_t$ is communicated with the parameter server.}
  \label{algo:model_averaging_worker}
  \begin{algorithmic}[1]
    \Procedure{ModelAveragingWorker}{$k$}
    \State $\theta^k_0 \gets \Call{Pull()}{}$ or $\Call{Random()}{}$ \Comment{Worker variable $\theta^k_0$ can be randomized.}
    \State $t \gets 0$
    \While{$\textbf{not}$ converged}
    \State $i \gets 0$
    \For{$i < \lambda$}
    \State $\textbf{x},~\textbf{y} \gets \Call{FetchNextMiniBatch()}{}$
    \State $\theta^k_{t + 1} \gets \theta^k_t - \eta_t \odot \nabla_\theta \mathcal{L}(\theta^k_t;\textbf{x};\textbf{y})$ \Comment{Optimization step, could be \cite{kingma2014adam}, or other optimizer.}
    \State $i \gets i + 1$
    \State $t \gets t + 1$
    \EndFor
    \State $\Call{Commit}{\theta^k_t}$
    \State $\Call{WaitForOtherWorkers()}{}$
    \State $\theta^k_t \gets \Call{Pull()}{}$
    \EndWhile
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\vspace*{1cm}

\begin{figure}[H]
  \centering
  %% 1
  \begin{tikzpicture}
    % Draw the axis of the plots.
    \draw[->] (0,0) -- (12,0) node[midway, below] {$\theta$};
    \draw[->] (0,0) -- (0, 2) node[midway, left] {$J(\theta)$};
    % Draw the hypothesis space, which is sin(x) / 2 + 0.55
    \draw[samples=400,domain=0:11.8,smooth,variable=\x,black,thick]  plot ({\x},{sin(deg(\x * 2)) / 1.5 + 0.7});
    % Draw the instances, and their gradient arrows.
    \draw (1,1.8) node[circle,inner sep=2pt,draw=black!50] {1};
    \draw[dashed, draw=black!30] (1,1.28) -- (1,1.55);
    \draw[->] (1.3,1.8) -- (2,1.8) node[midway, above] {\small $\Delta\theta_1$};

    \draw (3.4,1.8) node[circle,inner sep=2pt,draw=black!50] {2};
    \draw[dashed, draw=black!30] (3.4,1.1) -- (3.4,1.55);
    \draw[->] (3.1,1.8) -- (2.2,1.8) node[midway, above] {\small $\Delta\theta_2$};

    \draw (7,1.8) node[circle,inner sep=2pt,draw=black!50] {3};
    \draw[dashed, draw=black!30] (7,1.4) -- (7,1.55);
    \draw[->] (6.7,1.8) -- (6.3,1.8) node[midway, above] {\small $\Delta\theta_3$};

    \draw (10,1.8) node[circle,inner sep=2pt,draw=black!50] {4};
    \draw[dashed, draw=black!30] (10,1.35) -- (10,1.55);
    \draw[->] (9.7,1.8) -- (9.15,1.8) node[midway, above] {\small $\Delta\theta_4$};
  \end{tikzpicture}
  %% 2
  \begin{tikzpicture}
    % Draw the axis of the plots.
    \draw[->] (0,0) -- (12,0) node[midway, below] {$\theta$};
    \draw[->] (0,0) -- (0, 2) node[midway, left] {$J(\theta)$};
    % Draw the hypothesis space, which is sin(x) / 2 + 0.55
    \draw[samples=400,domain=0:11.8,smooth,variable=\x,black,thick]  plot ({\x},{sin(deg(\x * 2)) / 1.5 + 0.7});
    % Draw the instances, and their gradient arrows.
    \draw (1.8,1.8) node[circle,inner sep=2pt,draw=black!50] {1};
    \draw[dashed, draw=black!30] (1.8,0.4) -- (1.8,1.6);

    \draw (2.7,1.8) node[circle,inner sep=2pt,draw=black!50] {2};
    \draw[dashed, draw=black!30] (2.7,0.3) -- (2.7,1.6);

    \draw (6.6,1.8) node[circle,inner sep=2pt,draw=black!50] {3};
    \draw[dashed, draw=black!30] (6.6,1.2) -- (6.6,1.6);

    \draw (9.4,1.8) node[circle,inner sep=2pt,draw=black!50] {4};
    \draw[dashed, draw=black!30] (9.4,0.8) -- (9.4,1.6);
    % Draw the center variable.
    \draw (5.125,1.8) node[circle,inner sep=2pt, draw=black!50] {$C$};
    \draw[dashed, draw=black!30] (5.125,0.3) -- (5.125,1.5);
  \end{tikzpicture}
  %% 3
  \begin{tikzpicture}
    % Draw the axis of the plots.
    \draw[->] (0,0) -- (12,0) node[midway, below] {$\theta$};
    \draw[->] (0,0) -- (0, 2) node[midway, left] {$J(\theta)$};
    % Draw the hypothesis space, which is sin(x) / 2 + 0.55
    \draw[samples=400,domain=0:11.8,smooth,variable=\x,black,thick]  plot ({\x},{sin(deg(\x * 2)) / 1.5 + 0.7});
    % Draw the instances, and their gradient arrows.
    \draw (5.125,1.8) node[rectangle,rounded corners,inner sep=4pt, draw=black!50] {$1,2,3,4$};
    \draw[dashed, draw=black!30] (5.125,0.3) -- (5.125,1.5);
    \draw[->] (5.95,1.8) -- (7,1.8) node[midway, above] {\small $\Delta\theta_{1-4}$};
  \end{tikzpicture}
  \caption{This figure explains the intuition behind model averaging. In the first state, all workers, $w_1$, $w_2$, $w_3$, and $w_4$, are uniformly initialized over the hypothesis space. Using the local parametrizations, every worker obtains an update $\Delta w_i$, and applies it locally. Afterwards, all workers send their parametrizations to the parameter server which will average them to obtain a central variable, which is depicted by $C$ in this particular figure. Finally, all workers fetch the most recent central variable, and start computing new gradients based for the following iteration.  Furthermore, what can be observed directly from this figure, is that when the workers do not agree on a \emph{local neighborhood}, the central variable will not be able to converge. This is additional support for Hypothesis~\ref{hyp:local_optimization}.}
  \label{fig:model_averaging_intuition}
\end{figure}

Nevertheless, what is interesting about the random initialization of workers, is that when we increase the number of workers, the probability that we will find a better solution (minima) compared to a sequential optimization process increases. However, this also depends on the curvature of the error space. For example, in Figure~\ref{fig:model_averaging_prob} we use Beale's function to obtain our statistic. However, from the plots we can deduce that the curvature of the error space is slightly biased towards the global minimum if first-order gradients are used. If the error space was not biased towards a specific minima, the statistic for a single worker under different hyperparameterizations should be 50\%.\\

Furthermore, what is the role of the exploration parameter $\lambda$? Does it contribute to to the optimization process besides reducing the amount of communication with the parameter server by increasing the amount of local work? In principle this would help to optimization process since more exploration of the parameter space occurs, and as a result, better updates are applied. Remember what we said in Section~\ref{sec:ddl_introduction} on synchronous data parallelism, that effectivly increasing the number of workers implicitly increases the size of the mini-batch. Yet, in this particular case there is a subtle difference, i.e., local exploration of the parameter space. As a result, it is not (really) implicitly increasing the size of the mini-batch since some form of local exploration occurs. As a result, \emph{the averaged central variable will produce a less-noisy consensus based on the (local) exploration of the error space}.  However, the problem lies in the fact when different sets of workers enter different minima, possible because of too much exploration of the error space as depicted in Figure~\ref{fig:model_averaging_intuition}. Because if this occurs, then the averaging step could potentially reset the situation instead of continuing exploring a single minima.

\begin{figure}
  \centering
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{resources/images/model_averaging_prob_1}
    \caption{$\lambda = 1$}
  \end{subfigure}
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{resources/images/model_averaging_cf_100_prob}
    \caption{$\lambda = 100$}
  \end{subfigure}
  \caption{Probability distribution extracted from several Monte-Carlo simulations under different conditions. We find that the probability of reaching the optimum (Beale's function) increases when the number of random initialized workers increases. Despite the fact that we observe that more exploration ($\lambda$) yiels a better statistic, we believe that this result is not significant due to the relatively low number of simulations (1000 per worker per hyperparameter).}
  \label{fig:model_averaging_prob}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{subfigure}{.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{resources/images/model_averaging_cf_1}
    \caption{$\lambda = 1$}
  \end{subfigure}
  \begin{subfigure}{.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{resources/images/model_averaging_cf_5}
    \caption{$\lambda = 5$}
  \end{subfigure}
  \begin{subfigure}{.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{resources/images/model_averaging_cf_100}
    \caption{$\lambda = 100$}
  \end{subfigure}
  \begin{subfigure}{.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{resources/images/model_averaging_cf_1000}
    \caption{$\lambda = 1000$}
  \end{subfigure}
  \label{fig:model_averaging_cf}
  \caption{Workers which have been randomly initialized (experiments use same initialization) with different values for $\lambda$ (communication frequency, or number of local worker iterations). Since the workers are initially randomized of the hypothesis space, the central variable in the first averaging step will be shifted towards the global minima because of the amount of local exploration, and due to the bias of the error space as shown in Figure~\ref{fig:model_averaging_prob}.}
\end{figure}

\subsection{Elastic Averaging SGD}
\label{sec:easgd}

Elastic Averaging SGD, or \textsc{easgd}~\cite{zhang2015deep}, is a distributed optimization algorithm designed with communication constraints in mind. In essence, \textsc{easgd} could be viewed as an extension of model averaging described in Section~\ref{sec:model_averaging} with the difference that the workers, and the central variable are coordinated using an \emph{elastic force}~\cite{zhang2015deep} instead of averaging the workers after a fixed amount of steps. This means that instead of simply transmitting the parametrization or a gradient to the parameter server, the workers commit an \emph{elastic difference} which is defined as $\eta_t \rho (\theta^k_t - \tilde{\theta}_t)$ where $\rho$ is the \emph{elasticity} hyperparameter. Intuitively, $\rho$ describes the amount of exploration that can be performed by the workers with respect to the central variable $\tilde{\theta}_t$.\\

Assuming $\lambda = 1$, the worker update and central variable update are described in Equation~\ref{eq:easgd_sync_worker}, and Equation~\ref{eq:easgd_sync_ps} respectively. What is different compared to most other optimization schemes, is that the worker update described in Equation~\ref{eq:easgd_sync_worker} has a second component, i.e., the \emph{elastic difference}. Furthermore, note that compared to other distributed optimization schemes, the workers in \textsc{easgd} do not synchronize their parameterization with the central variable as shown in Algorithm~\ref{algo:easgd_worker}, but rather update the central variable using the elastic difference, and than use the new central variable as a new reference point to compute the following elastic differences.

\begin{equation}
  \label{eq:easgd_sync_worker}
  \theta^k_{t + 1} = \theta^k_t - \eta_t \odot \nabla_\theta \mathcal{L}(\theta^k_t;\mathbf{x}^k_t;\mathbf{y}^k_t) - \eta_t\rho(\theta^k_t - \tilde{\theta}_t)
\end{equation}

\begin{equation}
  \label{eq:easgd_sync_ps}
  \tilde{\theta}_{t+1} = \tilde{\theta}_t + \eta_t \sum_{i=0}^n \rho(\theta^i_t - \tilde{\theta}_t)
\end{equation}

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    % Draw the axis of the plots.
    \draw[->] (1,0) -- (9,0) node[midway, below] {$\theta$};
    \draw[->] (1,0) -- (1,2) node[midway, left] {$J(\theta)$};
    % Draw the function.
    \draw[samples=400,domain=1:8.8,smooth,variable=\x,black,thick]  plot ({\x},{(1 / (\x - 0.45))});
    % Draw the instances (including the center variable) with the gradient arrows and the elastic difference.
    \draw (2, 1.8) node[circle, inner sep=2pt, draw=black!50] {$\tilde{\theta}$};
    \draw[dashed, draw=black!30] (2,0.65) -- (2,1.5);
    \draw (7, 1.8) node[circle, inner sep=2pt, draw=black!50] {$\theta_w$};
    \draw[dashed, draw=black!30] (7,0.25) -- (7,1.45);
    % Draw the gradients and the elastic force.
    \draw[->] (7.45,1.8) -- (8.1,1.8) node[right, above] {\small $-\nabla_\theta \mathcal{L}_w$};
    \draw[->] (6.55,1.55) -- (6.2,1.55) node[midway, below] {\small $-\eta\rho(\theta_w - \tilde{\theta})$ (small $\rho$)};
    \draw[->] (6.55,2.05) -- (5.6,2.05) node[midway, above] {\small $-\eta\rho(\theta_w - \tilde{\theta})$ (large $\rho$)};
    % Draw elastic force applied to the central variable.
    \draw[->] (2.45, 1.8) -- (3.6, 1.8) node[midway, above] {\small $\eta\rho(\theta_w - \tilde{\theta})$};
  \end{tikzpicture}
  \caption{In this example we would like to make the intuition behind Elastic Averaging SGD clear by describing it as a classical physics problem. In this particular figure we have 1 worker, $\theta_w$ ($n = 1$, $\lambda = 1$), and a central variable $\tilde{\theta}$. The $y$-axis represents the error with respect to a certain parameterization $\theta$. Using Algorithm~\ref{algo:easgd_worker}, the worker performs $\lambda$ steps to compute the next value of $\theta_w$ using Equation~\ref{eq:easgd_sync_worker}. As stated before in Section~\ref{sec:easgd}, Equation~\ref{eq:easgd_sync_worker} holds 2 components, i.e., the regular negated first-order gradient ($-\eta_t \nabla_\theta \mathcal{L}_w$), and the \emph{negated} elastic difference ($-\eta_t\rho(\theta_w - \tilde{\theta})$). As always, the negated first-order gradient represents the steepest slope with respect to the current error (loss), and parameterization. However, the \emph{negated} elastic difference actually points towards the central variable. The magnitude of the elastic difference is controlled by hyperparameter $\rho$. This implies that a large value of $\rho$ \emph{strongly} limits the amount of exploration a worker can perform with respect to the central variable since the magnitude of the elastic difference vector will be proportionally larger compared to a small $\rho$. Finally, the central variable is updated using Equation~\ref{eq:easgd_sync_ps} and the elastic difference coming from all workers. Furthermore, we would like to note that the elastic difference during the central variable update is \emph{not} negated. Meaning that the central variable is optimized with respect to the ``pull'' of all workers, with the effect that workers whichare ahead in the optimization process, will have a larger elastic force, causing the central variable to be influenced more strongly by distant workers. Nevertheless, at the same time, distant workers are pulled towards the central variable using an equal but opposite force \emph{and} the negated gradient, i.e., the net force of the elastic difference and the negated gradient combined results either in a step towards a minimum, a null-operation, or a step in the direction of the central variable.}
\end{figure}

\begin{algorithm}[H]
  \caption{Worker procedure of synchronous \textsc{easgd}. This algorithms accepts several hyperparameters, the first being the number of local computations $\lambda$, the exploration hyperparameter $\rho$, and the dynamic learning rate $\eta_t$.}
  \label{algo:easgd_worker}
  \begin{algorithmic}[1]
    \Procedure{EASGDWorker}{$k$}
    \State $\theta^k_0 \gets \tilde{\theta} \gets \Call{Pull}$
    \State $t \gets 0$
    \While{$\textbf{not}$ converged}
    \State $i \gets 0$
    \For{$i < \lambda$}
    \State $\textbf{x},~\textbf{y} \gets \Call{FetchNextMiniBatch()}{}$
    \State $\theta^k_{t + 1} \gets \theta^k_t - \eta_t \odot \nabla_\theta \mathcal{L}(\theta^k_t;\textbf{x};\textbf{y})$ \Comment{Optimization step, could be \cite{kingma2014adam}, or other optimizer.}
    \State $i \gets i + 1$
    \State $t \gets t + 1$
    \EndFor
    \State $\mathcal{E} = \eta_t\rho(\theta^k_t - \tilde{\theta})$
    \State $\theta^k_{t+1} = \theta^k_t - \mathcal{E}$
    \State $\Call{Commit}{\mathcal{E}}$
    \State $\Call{WaitForOtherWorkers()}{}$
    \State $\tilde{\theta} \gets \Call{Pull}$
    \State $t \gets t + 1$
    \EndWhile
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\section{Asynchronous Data Parallelism}
\label{sec:asynchronous_data_parallelism}

\subsection{DOWNPOUR}
\label{sec:downpour}

\subsection{Dynamic SGD}
\label{sec:dyn_sgd}

\subsection{Asychrony Induced Momentum}
\label{sec:implicit_momentum}

\subsection{Asynchronous Elastic Averaging SGD}
\label{sec:aeasgd}

\section{Hybrids}
\label{sec:hybrids}
