% Chapter on the conclusion.
%
% Developed for my Master Thesis at Maastricht University.
% Based on Eugenio Senes's template at the University of Torino.
%
% By Joeri Hermans (joeri@joerihermans.com)
%
% Released under an MIT license. Share, modify and enjoy, but quote the author!

\chapter{Conclusion}
\label{chapter:conclusion}

This work presents a detailed outline of the current landscape in distributed optimization with an application to Deep Learning. We started in Chapter~\ref{chapter:introduction} by giving a rough summary of different concepts, how they are currently applied, and their pitfalls. Furthermore, we introduced our first hypothesis, which states that \emph{workers optimize the central variable efficiently when they compute gradients based on older central variables which are close to the current central variable}, and was proved empirically on several occasions throughout this thesis. In addition,\\

While researching other distributed optimization methodologies, we found that \textsc{easgd}~\cite{zhang2015deep} and derrivates, have very interesting properties. The first property, which is present in all \textsc{easgd} derrived algorithms, is the presence of an \emph{equilibrium} condition. Meaning, points exist during the optimization process that any additional computations done by a worker are inconsequential. Furthermore, this equilibrium is especially problematic if the central variable, and workers are close to a minima, causing \textsc{easgd} to converge (very) slowly to the minima. However, this equilibrium has desired side-effects in some cases, because it presents the optimizer from overfitting, as shown emperically in our experiments. Furthermore, a second interesting observation was that in the asynchronous version of \textsc{easgd}, i.e., \textsc{aeasgd}, the workers are not \emph{pushing} the central variable towards a minima, as is the case in settings similar to \textsc{downpour}. But the workers rather act as a normal distribution around the central variable, i.e., the central variable as the mean of the distribution, with the variance of the distribution being proportional to the exploration hyperparameter $\rho$. Afterwards, we considered \textsc{dynsgd}~\cite{jiang2017heterogeneity}, which is an attempt to deal with staleness in a non-\emph{stale-synchronous} manner. However, using our hypothesis, we showed that the way \textsc{dynsgd} deals with staleness, which is in terms of stale steps, is a rather naive approach.\\

This resulted in the first contribution of this thesis, \textsc{agn}, which is described in Chapter~\ref{chapter:accumulated_gradient_normalization}. During the development of \textsc{agn}, we made the same \emph{practical} assumptions with respect to constraints as \textsc{easgd}, i.e., high communication costs. However, it was desired that the optimizer did not have the equilibrium issues \textsc{easgd} derrived algorithms had. As a result, we turned to \textsc{downpour}, and allowed for more local exploration by sending \emph{accumulated gradients} to the parameter server. However, this approach diverged even quicker than regular \textsc{downpour}, with the difference that data was processed significantly faster due to the reduced waits. As a result, \textsc{agn} was adapted to use the time between parameter updates more efficiently by computing a \emph{better} gradient based on a normalized sequence of first-order gradients. Furthermore, we showed that \textsc{agn} outperforms all currently existing distributed optimizers in terms of training, and validation accuracy in the presence of a large amount of concurrency and reduced communication frequency. In addition, as the number of asynchronous workers was increased, divergent behaviour started to occur. However, \textsc{agn} was able to stabalize in all situations. Nevertheless, this behaviour is not desired, as it impairs the convergence rate of the central variable.

\section{Research Questions}
\label{sec:conclusion_research_questions}

\section{Future Work}
\label{sec:conclusion_future_work}
